from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer, LTChar,LTTextLine
import logging
import more_itertools
import re
from itertools import groupby, count
from operator import itemgetter
from langdetect import detect
import os
import pickle

"""
All functions used to synthesize metadata from the content of the PDF.
Text & associated font_size information is extracted directly from the sourcecode
of the article's PDF by pdfminer.six (github.com/pdfminer/pdfminer.six). The extracted 
text tuple is stored as pickled csv files under folder "streams" as to avoid 
re-processing pdf sourcecode; most of the computation is spent reading the pdf's binaries.

Metadata synthesis is framed as a Constraint Satisfaction Problem (CSP) where input
tuple <text stream, font name, font size> must satisfy a set of constraints
to classify as one of [Start Page, End Page, Keywords, Submission Date, Abstract, 
References, and [En-Title & En-Author for Korean]] metadata fields. There may
be no solutions, in which the result is "None". 

English vs. Korean extraction processes (compiled in ko_main, en_main)
slightly differ due to the discrepancies in internal article structure 
& suboptimal constraints caused by differing graphical characteristics of each script.

"""


def extract_PDF_elements(PDF_path):
    """
    - input: path to PDF file
    - output: list of all tuples in article

    extract_PDF_elements() is used to extract build tuple 
    <text stream, font name, font size>. The font name & size
    of each individual character is generated by iterating 
    through LTTextBox, LTTextLine, then LTChar. 
    
    See pdfminer.six's layout analysis algorithm at
    https://pdfminersix.readthedocs.io/en/latest/topic/converting_pdf_to_text.html#topic-pdf-to-text-layout
    
    See official documentation at
    https://pdfminersix.readthedocs.io/en/latest/tutorial/extract_pages.html
    """

    logging.getLogger('pdfminer').setLevel(logging.ERROR)
    
    extracted_data=[]
    for page_layout in extract_pages(PDF_path):
        for element in page_layout:
            if isinstance(element, LTTextContainer):
                for text_line in element:
                    if isinstance(text_line, LTTextLine):
                        for character in text_line:
                            if isinstance(character, LTChar):
                                font_size=character.size
                                font_name=character.fontname
                extracted_data.append([font_size,font_name.strip(),element.get_text().strip()])
    # remove blanks
    extracted_data = [e for e in extracted_data if e[2]]
    return extracted_data

def extraction_maker(PDF_path):
    """
    - input: path to PDF
    - output: extracted tuples, pickled csv file of extracted tuples

    Calls extract_PDF_elements() to write the extracted 
    content into the appropriate directory under folder 
    "streams". It takes ~0.4 sec to extract info 
    per article, while the remaining algo takes ~0.003 sec. 
    Simply avoids having to parse again. Useful for 
    corpus-oriented tasks. 

    """
    directory = PDF_path[:-4]
    directory = directory.split("journals")[1]
    csv_directory,md5hash = directory.rsplit('/', 1)
    csv_directory="streams/"+csv_directory+"/"
    csv_path = csv_directory+md5hash+".csv"

    # make directory to save in
    if not os.path.exists(csv_directory):
        os.makedirs(csv_directory) 

    extracted_data = extract_PDF_elements(PDF_path)
    # write extracted text into csv file
    with open(csv_path, 'wb') as f:
        pickle.dump(extracted_data, f)
    return extracted_data

def page_numbers(extracted_data):
    """
    - input: list of all tuples
    - output: page range & spliced list of tuples

    Both English & Korean articles share page number
    a page number format of "－ 79 －". A length 3 sliding 
    window is applied over a splitted text stream to check
    this format. If True, the page number is 
    extracted & stored but removed from the contenders to 
    avoid false calls whilst checking for other metadata 
    classes. Returns minimum & maximum page numbers to
    obtain the range for the article as part of referencing
    standards. 
    """
    # variables
    minpageno = None
    maxpageno = None
    pagenos = []
    pageno_idx = []
    for i,e in enumerate(extracted_data):
        splitted = str(e[2]).replace("-","－").split()
        window_size = 3
        for ii in range(len(splitted) - window_size + 1):
            window = splitted[ii: ii + window_size]
            try:
                if window[0]== "－" and window[1].isdigit() and window[2] == "－":
                    pageno_idx.append(i)
                    pagenos.append(int(window[1]))
                else:
                    continue
            except:
                continue
    minpageno = min(pagenos,default=None) # default=None resolves errors from "None"
    maxpageno = max(pagenos,default=None)
    spliced_data = [i for j, i in enumerate(extracted_data) if j not in pageno_idx]
    return minpageno,maxpageno,spliced_data

def ko_extract(extracted_data):
    """
    - input: list of all tuples
    - output: english title, english author, abstract, keyword, submission date


    Main extraction method for all Korean articles. All 5 fields occur 
    in the end of the article after "references" field. ko_extract() 
    is called when there has been references, and consequently uses tuples 
    spliced after the occurrence of all references; this signals the evidence 
    for the existence of these fields and limits search boundaries,
    creating a probable likelihood of accurate detection. 

    First, keyword & subdate are detected by a relatively easy inclusion
    constraint. Since these fields are all clumped in some form in the end of
    the article, the order of tuples is reversed to find the first instance 
    of the inclusion. If found, the indices are removed.

    The main difficulty arises in the need to distinguish the title, author,
    and abstract, all of which do not have explicit "signal words" and are
    not restricted to a certain stream length, i.e. the title may be multi-lined.
    The three are distinguished through a set of meta characteristics including 
    text length, occurrence order, font type, and font size. 
    
    For instance, precheck for author, title, and abstract detection checks for the
    "best case scenario" where there exists a stream with "Italic" in the font followed 
    by a stream with "Bold" in the font indicating author and title, respectively. 
    If so, the two indices are again removed, meaning only the streams of the abstract 
    remain. As a validation step, the guessed indices of the abstract are checked for 
    continuity since they may be multi-lined, but cannot be discontinous indices. 
    Page numbers, as above, are removed for constraints as such.

    """
    # variables
    title = None
    author = None
    abstract = None
    keyword = None
    subdate = None
    
    # reversed search
    reversed_extracted_data = extracted_data[::-1]
    
    # 5 classes are:
    #####################################################
    ## "원고접수","author","title","abstract","keywords" ##
    #####################################################

    # (1)finding keywords
    for i,run in enumerate(reversed_extracted_data):
        if run[2]:
            for signal_word in ["실마리어","Keywords :","Key words :","Keywords:","Key words:","Key  words:","Keyword:","Key word:"]:
                if signal_word not in str(run[2]):
                    continue
                elif signal_word in str(run[2]):
                    element = run[2].strip()
                    # on its own
                    if element.startswith(signal_word):
                        kw = element.split(signal_word)[1]
                        keyword = kw.split(",")
                        keyword = [k.strip() for k in keyword]
                        del reversed_extracted_data[i]
                        break
                    # embedded in other line
                    else:
                        instance,kw = str(run[2]).split(signal_word,1)
                        keyword = kw.split(",")
                        keyword = [k.strip() for k in keyword]
                        if instance:
                            reversed_extracted_data[i][2] = instance
                            break
                            
    # (2) finding submission date  
    for i,run in enumerate(reversed_extracted_data):
        if run[2]:
            if "원고접수" in str(run[2]):
                subdate = str(run[2]).split("원고접수")[0]
                del reversed_extracted_data[i]
                break
        else:
            continue
            
            
    # (3) finding title & authors 
    for i,run in enumerate(reversed_extracted_data):
        if run[2]:
            if str(run[2]).strip().startswith("\uf113\uf114\uf115종합대학학보"):
                del reversed_extracted_data[i]
                break

    idx_list = list(range(len(reversed_extracted_data)))
    
    ####################################################
    ################ pre check  ########################
    ####################################################
    
            ###############################
            # italics == author
            # bold == almost always title
            ###############################
    
    if idx_list:
        precheck = [i for i in idx_list[:-1] if "Italic" in str(reversed_extracted_data[i][1]) and "Bold" in str(reversed_extracted_data[i+1][1])]
        if precheck:
            if len(precheck) == 1:
                author = str(reversed_extracted_data[precheck[0]][2].strip())
                title = str(reversed_extracted_data[precheck[0]+1][2].strip())
                # delete precheck from list
                idx_list = [i for i in idx_list if i not in [precheck[0],precheck[0]+1]]

                # remaining == abstract
                # gotta be consecutive indices; 
                # unless there's one left
            
                if len(idx_list) < 1:
                    return keyword,subdate,author,title,abstract
            
                elif len(idx_list) == 1:
                    abstract = str(reversed_extracted_data[idx_list[0]][2])
                    return keyword,subdate,author,title,abstract
            
                elif len(idx_list) == 2:
                    if idx_list == largest_consecutive(idx_list):
                        abstract = [str(reversed_extracted_data[ii][2]) for ii in idx_list]
                        abstract = abstract[::-1] ## reverse it
                        abstract = " ".join(abstract)
                        return keyword,subdate,author,title,abstract
                # filter out; get longest one only if nonconsecutive
                    else:
                        abstract_idx = 0 if len(reversed_extracted_data[idx_list[0]][2]) > len(reversed_extracted_data[idx_list[1]][2]) else 1
                        abstract = reversed_extracted_data[abstract_idx][2]
                        return keyword,subdate,author,title,abstract
                
            #find largest continous group of integers in index list
                elif check_continuity(idx_list):
                    abstract = [str(reversed_extracted_data[ii][2]) for ii in idx_list]
                    abstract = abstract[::-1] ## reverse it
                    abstract = " ".join(abstract)
                    return keyword,subdate,author,title,abstract
            
                else:
                    grouped = group_consecutive_ints(idx_list)
                    grouped = [g for g in grouped if not is_list_float([str(reversed_extracted_data[ii][2]) for ii in g])]
                    grouped_strings = []
                    for g in grouped:
                        grouped_strings.append([str(reversed_extracted_data[ii][2]) for ii in g][::-1])
                    abstract = max([" ".join(g) for g in grouped_strings], key=len)
                    return keyword,subdate,author,title,abstract

    ################################################
    ## did not pass precheck; iterate through all
    ################################################
    
    ## if continous & length == 3, there's only 3 objects left so,
    if len(idx_list) == 3 and check_continuity(idx_list):
        bolds = [i for i in idx_list if "Bold" in str(reversed_extracted_data[i][1])] 
        # only title
        
        if len(bolds) == 1:
            title = str(reversed_extracted_data[bolds[0]][2])
            del idx_list[bolds[0]]
            candidate1 = str(reversed_extracted_data[idx_list[0]][2])
            candidate2 = str(reversed_extracted_data[idx_list[1]][2])
            author_i = 1 if len(candidate1) > len(candidate2) else 0
            author = str(reversed_extracted_data[idx_list[author_i]][2])
            del idx_list[author_i]
            abstract = str(reversed_extracted_data[idx_list[0]][2])
            return keyword,subdate,author,title,abstract
        
        elif len(bolds) == 2:
            candidate1 = str(reversed_extracted_data[bolds[0]][2])
            candidate2 = str(reversed_extracted_data[bolds[1]][2])
            author_i = 1 if len(candidate1) > len(candidate2) else 0
            author = str(reversed_extracted_data[bolds[author_i]][2])
            del bolds[author_i]
            title = str(reversed_extracted_data[bolds[0]][2])
            abstract = str(reversed_extracted_data[bolds[0]][2])
            return keyword,subdate,author,title,abstract
        
   
    for i in idx_list[:-1]:
        try:
            run = reversed_extracted_data[i]
            if "Italic" in str(run[1]) and "Bold" in str(reversed_extracted_data[i+1][1]):
                ## guaranteed case; both author & title
                author = str(run[2]).strip()
                title = str(reversed_extracted_data[i+1][2].strip())
                del idx_list[i:i+1]
                # remaining == abstract
                # gotta be continous; find largest continous group of integers in index list
                abstract_idx = max([list(group) for group in more_itertools.consecutive_groups(idx_list)], key=len)
                abstract = [str(reversed_extracted_data[ii][2]) for ii in abstract_idx]
                abstract = abstract[::-1] ## reverse it
                abstract = " ".join(abstract)
                return keyword,subdate,author,title,abstract

            elif "Bold" in str(run[1]) and i+1 != len(idx_list) and "Bold" in str(reversed_extracted_data[i+1][1]):
                
                candidate1 = str(reversed_extracted_data[idx_list[i]][2])
                candidate2 = str(reversed_extracted_data[idx_list[i+1]][2])
                candidate_idx = [i,i+1]
                idx_list = [ci for ci in idx_list if ci not in candidate_idx]
                title = candidate1 if len(candidate1) > len(candidate2) else candidate2
                author = candidate2 if candidate1 == title else candidate1
                if check_continuity(idx_list):
                    abstract = [str(reversed_extracted_data[ii][2]) for ii in idx_list]
                    abstract = abstract[::-1] ## reverse it
                    abstract = " ".join(abstract)
                    return keyword,subdate,author,title,abstract
                
            elif "Bold" in str(run[1]):
                # only title is formatted right
                title = str(run[2]).strip()
                del idx_list[i]
                # differentiate title from abstract
                if len(idx_list) == 2:
                    # length abstract is always > author
                    # only two idx left, so deduce from length
                    candidate1 = str(reversed_extracted_data[idx_list[0]][2])
                    candidate2 = str(reversed_extracted_data[idx_list[1]][2])
                    
                    if len(candidate1) > len(candidate2):
                        abstract = candidate1
                        author = candidate2
                        return keyword,subdate,author,title,abstract
                    else:
                        abstract = candidate2
                        author = candidate1
                        return keyword,subdate,author,title,abstract
                
                elif len(idx_list) > 2:
                    if check_continuity(idx_list):
                    # deduce
                        candidate1 = str(reversed_extracted_data[idx_list[0]][2])
                        candidate2 = str(reversed_extracted_data[idx_list[-1]][2])
                        author_i = -1 if len(candidate1) > len(candidate2) else 0
                        if len(reversed_extracted_data[idx_list[author_i]][2]) < len(title):
                            author = str(reversed_extracted_data[idx_list[author_i]][2])
                            del idx_list[author_i]
                            abstract = [str(reversed_extracted_data[ii][2]) for ii in idx_list]
                            abstract = abstract[::-1] ## reverse it
                            abstract = " ".join(abstract)
                            return keyword,subdate,author,title,abstract
                    else:
                        if len(reversed_extracted_data[0][2]) > len(reversed_extracted_data[1][2]):
                            abstract = reversed_extracted_data[0][2]
                            author = reversed_extracted_data[1][2]
                            return keyword,subdate,author,title,abstract

            elif "Italic" in str(run[1]):
                # only author is formatted right
                author = str(run[2]).strip()
                del idx_list[i]
                # differentiate author from abstract
            
                if len(idx_list) == 2:
                    # length abstract is always > author
                    # only two idx left, so deduce from length
                    candidate1 = str(reversed_extracted_data[idx_list[0]][2])
                    candidate2 = str(reversed_extracted_data[idx_list[1]][2])
                    if len(candidate1) > len(candidate2):
                        abstract = candidate1
                        title = candidate2
                        return keyword,subdate,author,title,abstract
                    else:
                        abstract = candidate2
                        title = candidate1
                        return keyword,subdate,author,title,abstract
                
                elif len(idx_list) > 2:
                    abstract_idx = max([list(group) for group in more_itertools.consecutive_groups(idx_list)], key=len)
                    # author takes up one line; abstract can take up multiple
                    diff_list = list(set(idx_list) - set(abstract_idx))
                    if len(diff_list) == 1:
                        abstract = [str(reversed_extracted_data[ii][2]) for ii in abstract_idx]
                        abstract = abstract[::-1] ## reverse it
                        abstract = " ".join(abstract)
                        title = str(reversed_extracted_data[diff_list[0]][2]).strip()
                        return keyword,subdate,author,title,abstract
            else:
                continue

        except IndexError:
            break
        
    ## if continous & length > 3, deduce the 3 objects
    ## last resort
    if any(v is None for v in [keyword,subdate]):
        return keyword,subdate,author,title,abstract
    
    if len(idx_list) > 3 and check_continuity(idx_list): 
        bolds = [i for i in idx_list if "Bold" in str(reversed_extracted_data[i][1])]
        if len(bolds) == 1:
            title = str(reversed_extracted_data[bolds[0]][2])
            del reversed_extracted_data[bolds[0]]
            if is_author_format(reversed_extracted_data[bolds[0]-1]):
                author = reversed_extracted_data[bolds[0]-1][2]
                del reversed_extracted_data[bolds[0]-1]
                reversed_extracted_data = reversed_extracted_data[::-1] ## reverse it
                abstract = " ".join(r[2] for r in reversed_extracted_data)
                if "주체" not in abstract and "의" not in abstract:
                    return keyword,subdate,author,title,abstract
                else:
                    abstract = None 
                    return keyword,subdate,author,title,abstract
    return keyword,subdate,author,title,abstract

def ko_extract_nc(extracted_data):
    """
    - input: list of tuples
    - output: title, author, keyword, submission date

    Main extraction tool for Korean articles
    called iff references were not detected. This is 
    usually the case for humanities articles,
    which also do not tend to have English abstracts like
    science articles. Also, the probability of detecting
    the abstract is low; hence, the abstract is not sought.
    Uses similar techniques as ko_extract()
    """
    # variables
    title = None
    keyword = None
    subdate = None
    author = None
    

    # restrict to last %50
    n = round(len(extracted_data)*0.5)
    extracted_data = extracted_data[n:]
    reversed_extracted_data = extracted_data[::-1]

    for i,run in enumerate(reversed_extracted_data):
        if run[2]:
            for signal_word in ["실마리어","Key words :","Keywords:","Key words:","Key  words:","Keyword:","Key word:"]:
                if signal_word not in str(run[2]):
                    continue
                elif signal_word in str(run[2]):
                    element = run[2].strip()
                    # on its own
                    if element.startswith(signal_word):
                        kw = element.split(signal_word)[1]
                        keyword = kw.split(",")
                        keyword = [k.strip() for k in keyword]
                        del reversed_extracted_data[i]
                        break
                    # embedded in other line
                    else:
                        instance,kw = str(run[2]).split(signal_word,1)
                        keyword = kw.split(",")
                        keyword = [k.strip() for k in keyword]
                        if instance:
                            reversed_extracted_data[i][2] = instance
                            break
                            
    for i,run in enumerate(reversed_extracted_data):
        if run[2]:
            if "원고접수" in str(run[2]):
                subdate = str(run[2]).split("원고접수")[0]
                del reversed_extracted_data[i]
                break
        else:
            continue
    idx_list = list(range(len(reversed_extracted_data)))
    for i,run in enumerate(reversed_extracted_data):
        ####################################
        # italics == author
        # bold almost always == title
        ####################################
        try:
            if "Italic" in str(run[1]) and "Bold" in str(reversed_extracted_data[i+1][1]):
                ## guaranteed case; both author & title
                author = str(run[2]).strip()
                title = str(reversed_extracted_data[i+1][2].strip())

            elif "Bold" in str(run[1]) and "Bold" in str(reversed_extracted_data[i+1][1]):
                candidate1 = str(run[2])
                candidate2 = str(reversed_extracted_data[i+1][2])
                candidate_idx = [i,i+1]
                idx_list = [ci for ci in idx_list if ci not in candidate_idx]
                if len(candidate1) > len(candidate2):
                    title = candidate1
                    author = candidate2
                else:
                    title = candidate2
                    author = candidate1
                if check_continuity(idx_list):
                    abstract = [str(reversed_extracted_data[ii][2]) for ii in idx_list]
                    abstract = abstract[::-1] ## reverse it
                    abstract = " ".join(abstract)
                    return keyword,subdate,author,title,abstract

            else:
                continue
        except IndexError:
            break

    return keyword,subdate,author,title

def ko_references(extracted_data):
    """
    - input: list of all tuples
    - output: spliced list of tuples & list of references

    Extracts references from Korean articles. 

    References are a set of continous streams that
    begin with "[" usually after the occurrence of 
    "참고문헌", but do not strictly obey those rules 
    and are frequently embedded in other streams. 
    Total number of references are unknown beforehand, 
    hence subsequent streams are iterated in a while loop.

    Other metadata fields (e.g. abstract) tend to 
    occur after the references, so the range of references 
    is returned to limit false calls & maximize efficiency 
    of backtrack search. 

    """
    
    # precheck for efficiency
    runs = [i[2] for i in extracted_data]
    if all("[" not in run for run in runs):
        return None
    
    for i,run in enumerate(extracted_data):

        temp = []
        instance = 0 # count how many runs
        # find instance of keyword
        if any(s in run[2] for s in ("참  고  문  헌","참 고 문 헌","참고문헌")):
            if len (extracted_data[i:]) <=2:
                return None
            k = 1
            cidx = i
            j = i + k
            keepGoing = True
            while keepGoing == True and j < len(extracted_data):
                j = i + k
                try:
                    val = extracted_data[j][2].strip()
                    if not val: # skip if blank; not instance
                        k+=1
                        continue
                    # it is index
                    elif "[" in val:
                        # check if it indexes numbers [0]
                        vv = val.partition("[")[2].partition("]")[0]
                        if vv.isdigit():
                            temp.append(val)
                            k+=1
                            instance+=1 # update instance
                            continue
                        else:
                            keepGoing = False
                            break
                    # takes up multiple lines
                    elif "[" not in val and ("[" in extracted_data[j+1][2]):
                        if k == 1: # if not first one
                            instance+=1 # update instance
                            k+=1
                            keepGoing = True
                            continue
                        else:
                            temp.append(extracted_data[j][2])
                            k+=1
                            instance+=1 # update instance
                            keepGoing = True
                            continue
                    elif has_numbers(val) and val.endswith(".") and "[" not in val and "[" in extracted_data[j-1][2]:
                        temp.append(val)
                        k+=1
                        instance+=1 # update instance
                        keepGoing = False
                        break
                    # pass first one
                    elif k==1:
                        k+=1
                        instance+=1 # update instance
                        keepGoing = True
                        continue
                    # no longer idx
                    else:
                        keepGoing = False
                        break
                except:
                    keepGoing = False
            
            references = []
            for t in temp:
                references.append(t.split("\n"))
            references = [item for sublist in references for item in sublist]
            try:
                if extracted_data[i+instance+1:]:
                    return extracted_data[i+instance+1:],references
                
                if len (references) >3:
                    numbs = [substring_after(cit,"[") for cit in references if substring_after(cit,"[")]
                    digits = [idx[0] for idx in numbs]
                    duplicated = [idx for idx, x in enumerate(digits) if digits.count(x) > 1]
                    if duplicated:
                        references = references[:duplicated[0]+1]
                        cut_idx = [i for i,e in enumerate(extracted_data) if 1 == 1]
                        spliced_data = extracted_data[cidx+2:]
                        return spliced_data,references
                else:
                    return extracted_data[i+instance+1:],references
            except:
                return None,None
                
def en_extract_AK(extracted_data):
    """
    - input: list of tuples
    - output: abstract, keyword

    Searches for abstract & keywords in English articles.
    Case I of English article structures. This abstract & keyword 
    combo occurs consecutively usually in the front, hence the 
    search boundaries are limited to the first half of all streams; 
    there are no references in this structure. The other structure includes 
    keywords after references in the end of the article, but does not 
    contain abstracts. 

    Both keywords & abstracts are indicated by the occurrence of
    those two words in a stream, hence the main work done is deducing
    the accurate upper boundary to which a stream is a keyword or 
    abstract. Some heruistic checks like splicing up to the occurrence 
    of the word "Introduction" faciliates the search.

    """
    # variables
    abstract = None
    keyword = None

    # limit to first half
    extracted_data = extracted_data[:round(len(extracted_data)*0.5)]
    
    # snip intro
    for i,e in enumerate(extracted_data):
        if i>3 and e[2] == "Introduction":
            extracted_data = extracted_data[:i]
            break
            
    # find keywords first
    keyword_idx = None
    for i,e in enumerate(extracted_data):
        for signal_word in ["Key words ","Keywords","Key words","Key words ","Key  words","Keyword","Key word"]:
            if str(e[2]).startswith(signal_word):
                keyword_idx = i
                kw = str(e[2]).split(signal_word,1)[1].strip()
                kw = kw.split("\n",1)[0]
                keyword = kw.split(",")
                keyword = [k.strip() for k in keyword]
                break
            # embedded in other line
            elif signal_word in str(e[2]):
                keyword_idx = i+1
                keyword_list = str(e[2]).split(signal_word,1)
                extracted_data[i][2] = keyword_list[0]
                ## fix past words (part of abstract)
                if len(keyword_list) == 2:
                    kw = keyword_list[1].split("\n")
                    for ii,k in enumerate(kw):
                        if "Introduction" in k:
                            kw = kw[:ii]
                            break
                    kwidx = [i for i,k in enumerate(kw) if "," in k]
                    if len(kwidx) == 1:
                        keyword = kw[kwidx[0]]
                        keyword = keyword.split(",")
                        keyword = [k.strip() for k in keyword]
                        break
                    elif check_continuity(kwidx):
                        keyword = " ".join([kw[ii] for ii in kwidx])
                        keyword = keyword.split(",")
                        keyword = [k.strip() for k in keyword]
                        break
    abstract_idx = None
    if keyword_idx:
        extracted_data = extracted_data[:keyword_idx]
        for i,e in enumerate(extracted_data):
            if str(e[2]).lstrip() == "Abstract":
                abstract_idx = i+1
                abstract = extracted_data[abstract_idx:]
                abstract = [a[2].lstrip() for a in abstract]
                if abstract:
                    abstract = " ".join(abstract)
                    return abstract,keyword

            if str(e[2]).lstrip().startswith("Abstract"):
                extracted_data[i][2] = str(e[2]).lstrip().split("Abstract",1)[1]
                abstract_idx = i
                abstract = extracted_data[abstract_idx:]
                abstract = [a[2].lstrip() for a in abstract]
                if abstract:
                    abstract = " ".join(abstract)
                    return abstract,keyword
    if not keyword_idx:
        for i,e in enumerate(extracted_data):
            if str(e[2]).lstrip() == "Abstract" or str(e[2]).endswith("Abstract"):
                abstract_idx = i+1
                abstract_cand = extracted_data[abstract_idx:]
                for ii,aa in enumerate(abstract_cand):
                    if "1. Introduction" in aa[2]:
                        abstract = abstract_cand[:ii]
                        abstract = [a[2] for a in abstract]
                        return abstract,keyword
    return abstract,keyword

def en_extract_KC(extracted_data):
    """
    - input: list of tuples
    - output: references, keyword

    Case II of English articles. Includes references followed 
    by keywords in the very end of the article. Hence, search 
    boundaries are limited to the last half of all streams. 
    There are only two English journal publications, and it has 
    been found that they occur in the very end, bounded by the 
    keyword stream. Regerence streams are similarly indicated by "[". 
    However, unlike their fellow Korean reference streams, the streams
    are structured significantly cleaner and are bounded by the 
    keyword stream. Hence, the search simply looks for all subsequent 
    streams with inclusion of "[" after any occurrence of the variation 
    of "keyword". 
    
    """
    
    # variables
    references = None
    keyword = None
    # limit to last half
    extracted_data = extracted_data[-round(len(extracted_data)*0.5):]
    ########################## finding keywords ###################################
    for i,run in enumerate(extracted_data):
        if run[2]:
            for signal_word in ["Key words :","Keywords:","Keywords :","Keywords:","Key words:","Key  words:","Keywords ;","Keyword:","Key word:"]:
                if signal_word not in str(run[2]):
                    continue
                elif signal_word in str(run[2]):
                    element = run[2].strip()
                    # on its own
                    if element.startswith(signal_word):
                        kw = element.split(signal_word)[1]
                        keyword = kw.split(",")
                        keyword = [k.strip() for k in keyword]
                        del extracted_data[i]
                        break
    for i,run in enumerate(extracted_data):
        temp = []
        # finding references
        if run[2].startswith("References"):
            extracted_data[i][2] = run[2].split("References",1)[1]
            idxs = [ii for ii,v in enumerate(extracted_data[i:]) if "[" in v[2]]
            extracted_data = extracted_data[i:]
            windows = list(more_itertools.windowed(idxs, n=2, step=1))
            last = windows[-1][-1]
            temp = []
            for j,k in windows:
                v = ' '.join(str(v[2]) for v in extracted_data[j:k])
                temp.append(v)
            temp.append(' '.join(str(v[2]) for v in extracted_data[last:]))
            references = [element for item in temp for element in item.split('\n')]
            references = [c.strip() for c in references if c!= " "]
            return references,keyword
        
        elif "References" in run[2]:
            ## embedded in other line
            r = run[2].split("References",1)[1]
            temp.append(r.strip())
            i+=1
            vals = [v for ii,v in enumerate(extracted_data[i:]) if "[" in v[2]]
            idxs = [ii for ii,v in enumerate(extracted_data[i:]) if "[" in v[2]]
            extracted_data = extracted_data[i:]
            windows = list(more_itertools.windowed(idxs, n=2, step=1))
            last = windows[-1][-1]
            for j,k in windows:
                v = ' '.join(str(v[2]) for v in extracted_data[j:k])
                temp.append(v)
            temp.append(' '.join(str(v[2]) for v in extracted_data[last:]))
            references = [element for item in temp for element in item.split('\n')]
            references = [c.strip() for c in references if c!= " "]
            return references,keyword
    return references,keyword

###########################################################################################


def ko_main(path):
    """
    input: path to PDF file
    output: dictionary of title, author, keyword, submission date, references
            start page, end page, abstract

    Compiles all processses of text processing for Korean articles.

    """
    # variables
    Keyword = None
    SubDate = None
    EnAuthor = None
    EnTitle = None
    EnAbstract = None
    References = None

    # reading text tuples from pdf or loading from csv
    if path[-3:] == "pdf":
        extracted_content = extraction_maker(path)
    
    elif path[-3:] == "csv":
        with open(path, 'rb') as f:
            extracted_content = pickle.load(f)
    
    extracted_content = clean_text(extracted_content)
    
    PG = page_numbers(extracted_content)
    if PG:
        StartPage = PG[0]
        EndPage = PG[1]
        extracted_content = PG[2]

    C = ko_references(extracted_content)
    extracted_content = resolve_spacing(extracted_content)
    if C:
        c_removed = C[0]
        References = C[1]
        if c_removed:
            EC = ko_extract(c_removed)
            if EC:
                Keyword = EC[0]
                SubDate = EC[1]
                EnAuthor = EC[2]
                EnTitle = EC[3]
                EnAbstract = EC[4]
    else:
        ENC = ko_extract_nc(extracted_content)
        if ENC:
            References = None
            Keyword = ENC[0]
            SubDate = ENC[1]
            EnAuthor = ENC[2]
            EnTitle = ENC[3]
            EnAbstract = None

    compiled = {"En-Title":EnTitle,"En-Author":EnAuthor, 
                    "Start Page":StartPage,"End Page":EndPage, "Keywords":Keyword,
                    "Submission Date":SubDate,"Abstract":EnAbstract, "References":References}
    validated_content = ko_validate(compiled)
    
    return validated_content

def en_main(path):
    """
    input: path to PDF file
    output: dictionary of keyword, submission date, abstract,
            references, start page, end page

    Compiles all processses of text processing for English articles.
    
    """
    # variables
    Keyword = None
    References = None
    Abstract = None
    StartPage = None
    EndPage = None

    # reading text tuples from pdf or loading from csv
    if path[-3:] == "pdf":
        extracted_content = extraction_maker(path)
    
    elif path[-3:] == "csv":
        with open(path, 'rb') as f:
            extracted_content = pickle.load(f)

    extracted_content = clean_text(extracted_content)
    
    PG = page_numbers(extracted_content)
    if PG:
        StartPage = PG[0]
        EndPage = PG[1]
        extracted_content = PG[2]

    AK = en_extract_AK(extracted_content)
    if AK:
        Abstract = AK[0]
        Keyword = AK[1]
    KC = en_extract_KC(extracted_content)
    if KC:
        References = KC[0]
        if not Keyword:
            Keyword = KC[1]
    
    compiled = {"Start Page":StartPage,"End Page":EndPage,"Keywords":Keyword,
                    "Abstract": Abstract,"References":References}
    validated_content = en_validate(compiled)
    
    return validated_content



def ko_validate(content_dict):
    t = 'En-Title'
    if t:
        if len(t) > 200:
            content_dict['En-Title'] = None
        try:
            if detect(t) == "ko":
                content_dict['En-Title'] = None
        except:
            t=t
        
    specials = ["(",")","<",">",".","-","…","−"]
    # en author

    if 'En-Author' in content_dict:
        a = content_dict['En-Author']
        if a:
            if a.isdigit():
                content_dict['En-Author'] = None
            elif any(c in specials for c in a):
                content_dict['En-Author'] = None
            elif any(c in specials for c in a):
                content_dict['En-Author'] = None
            elif detect(a) == "ko":
                content_dict['En-Author'] = None
            elif len(a) == 1:
                content_dict['En-Author'] = None
        if a:
            a = a.replace("and",",").strip()
            if "," not in a and len(a) > 25:
                content_dict['En-Author'] = None
            else:
                a = a.replace("  "," ")
                a = a.split(",")
                a = [i.strip() for i in a]
                content_dict['En-Author'] = a
        content_dict['En-Author'] = a

    # abstract
    a = content_dict['Abstract']
    if a:
        if len(a) >= 1000:
            content_dict['Abstract'] = None
        try:
            if detect(a) == "ko":
                content_dict['Abstract'] = None
        except:
            a = a
    
    cit = content_dict['References']
    if cit:
        for i,c in enumerate(cit):
            cit[i] = c.strip()
            content_dict['References'] = cit

        for i,c in enumerate(cit):
            try:
                if not c.startswith("["):
                    if c.endswith(".") and cit[i-1].startswith("["):
                        cit[i-1] = cit[i-1]+" "+c
                        del cit[i]
                        content_dict['References'] = cit
            except IndexError:
                continue

        for i,c in enumerate(cit):
            try:
                if not c.startswith("["):
                    if c.endswith(".") and cit[i-1].startswith("["):
                        cit[i-1] = cit[i-1]+" "+c
                        del cit[i]
                        content_dict['References'] = cit
            except IndexError:
                continue

        for i,c in enumerate(cit):
            try:
                if not c.startswith("[") and not c.endswith("."):
                    if not cit[i+1].startswith("[") and cit[i+1].endswith("."): 
                        if cit[i-1].startswith("["):
                            cit[i-1] = cit[i-1]+" "+c+" "+cit[i+1]
                            del cit[i]
                            del cit[i]
                            content_dict['References'] = cit
            except IndexError:
                continue
                
        for i,c in enumerate(cit):
            try:
                if not c.startswith("[") and not c.endswith("."):
                    # sandwitch
                    if cit[i+1].startswith("[") and cit[i-1].startswith("["):
                        if not cit[i-1].endswith("."):
                            cit[i-1] = cit[i-1]+" "+c
                            del cit[i]
                            content_dict['References'] = cit
            except IndexError:
                continue
        for i,c in enumerate(cit):
            try:
                if not c.startswith("["):
                    if c.startswith("종합대학학보"):
                        if cit[i+1].startswith("[") and cit[i-1].startswith("["):
                            del cit[i]
                            content_dict['References'] = cit
            except IndexError:
                continue

        for i,c in enumerate(cit):
            try:
                if not c.startswith("["):
                    if c.endswith("원고접수"):
                        del cit[i]
            except IndexError:
                continue

        for i,c in enumerate(cit):
            try:
                if not c.startswith("["):
                    if c.endswith(".") and cit[i-1].startswith("["):
                        cit[i-1] = cit[i-1]+" "+c
                        del cit[i]
            except IndexError:
                continue

        for i,c in enumerate(cit):
            try:
                if not c.startswith("["):
                    if c.startswith("종합대학학보"):
                        del cit[i]
                        continue
                    elif "－" in c:
                        del cit[i]
                        continue
            except IndexError:
                continue
        for i,c in enumerate(cit):
            if not c.startswith("["):
                if len(cit) == 1:
                    content_dict['References'] = None
                continue

        for i,c in enumerate(cit):
            if not c.startswith("["):
                if "…" in c:
                    del cit[i]

        for i,c in enumerate(cit):
            try:
                if not c.startswith("[") and c.endswith("."):
                    cit[i-1] = cit[i-1] + " " + c
                    del cit[i]
                    continue
            except IndexError:
                continue

        for i,c in enumerate(cit):
            try:
                if not c.startswith("[") and c.endswith("."):
                    cit[i-1] = cit[i-1] + " " + c
                    del cit[i]
                    continue
            except IndexError:
                continue

        for i,c in enumerate(cit):
            try:
                if not c.startswith("["):
                    if c.startswith("종합대학학보"):
                        del cit[i]
                        continue
                    elif "－" in c:
                        del cit[i]
                        continue
            except IndexError:
                continue

        for i,c in enumerate(cit):
            try:
                if not c.startswith("["):
                    if c.endswith("원고접수"):
                        del cit[i]
            except IndexError:
                continue

        for i,c in enumerate(cit):
            try:
                if not c.startswith("["):
                    cit[i-1] = cit[i-1] +" "+c
                    del cit[i]
            except IndexError:
                continue
    cit = content_dict['References']
    
    # cit language
    if cit:
        temp = []
        for i,c in enumerate(cit):
            try:
                lang = detect(c)
                temp.append(lang)
                continue
            except:
                lang = None
                temp.append(lang)
                continue
        content_dict["References Language"] = temp
    else:
        content_dict["References Language"] = []

    return content_dict

def en_validate(content_dict):
    """
    input: dictionary extracted by en_main()
    output: cleaned dictionary + reference language

    Uses heruistic tests & natural language processing
    language detection tools to remove false calls.
    
    """

    # abstract
    a = content_dict['Abstract']
    if a:
        if isinstance(a, list):
            a = " ".join(a)
        else:
            a = a.replace("\n"," ")
            a = a.replace("  "," ")
        content_dict['Abstract'] = a
    
    # reference lang
    cit = content_dict['References']
    if cit:
        temp = []
        for c in cit:
            if c.isdigit():
                cit.remove(c)
                continue
            try:
                lang = detect(c)
                temp.append(lang)
            except:
                lang = None
                temp.append(lang)
                continue
        content_dict["References Language"] = temp
    else:
        content_dict["References Language"] = []
    return content_dict


def check_continuity(my_list):
    return all(a+1==b for a, b in zip(my_list, my_list[1:]))

def has_numbers(inputString):
    return any(char.isdigit() for char in inputString)

def is_list_float(stringlist):
    if all([str(i).replace(".", "", 1).isdigit() for i in stringlist]):
        return ''.join(map(str,stringlist))
    else:
        return False

def substring_after(s, delim):
    return s.partition(delim)[2]

def largest_consecutive(idx):
    c = count()
    consecutive = max((list(g) for _, g in groupby(idx, lambda x: x-next(c))), key=len)
    return consecutive

def group_consecutive_ints(idx):
    consecutives = [list(map(itemgetter(1), g)) for k, g in groupby(enumerate(idx), lambda ix : ix[0] - ix[1])]
    return consecutives

def clean_text(data):
    for i,e in enumerate(data):
        if e[2]:
            data[i][2] = str(e[2]).replace("［","[")
            data[i][2] = str(e[2]).replace("］","] ")
            data[i][2] = str(e[2]).replace("  "," ")
        else:
            continue
    return data

def resolve_spacing(data):
    for i,e in enumerate(data):
        if e[2]:
            data[i][2] = str(e[2]).replace("\n"," ")
            data[i][2] = str(e[2]).replace("  "," ")
        else:
            continue
    return data

def is_author_format(test):
    checks = r"()\.－"
    if any(elem in test for elem in checks):
        return False
    elif "," not in test and len(test)>=25:
        return False
    else:
        return True
