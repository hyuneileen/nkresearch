Journal of Kim Il Sung University (Natural Science) Vol. 3 No. 3 Juche103(2014) Asymptotic Properties of Variance Estimator in  Nonlinear Autoregressive Time Series Models with α－Mixing Errors Term  Kim Kyong Hui, Kim Ok Gyong Abstract We considered the asymptotic properties of the variance estimator of errors in nonlinear autoregressive time series models with −α mixing errors. The estimator based on the residuals is shown to be consistent for the error variance. And the asymptotic distribution of the variance estimator is proved to be normal. Key word nonlinear autoregressive model  The great leader Comrade Kim Il Sung said as follows. “We should actively develop the major areas of basic sciences such as mathematics, physics, chemistry and biology so as to raise the national standard of science and technology and find more effective solutions to the scientific and technical problems that arise in the different branches of the national economy.”(“KIM IL SUNG WORKS” Vol. 35 P. 313) Nonlinear autoregressive time series model is as follows. X i = ( Xr θ , 1 L − , i X ) + ε i pi − (1) Let { =iX i , Θ∈θθ ,r where ,0 ± ,1 ,2 } L± be a stationary sequence of real variables satisfying above model, is a family of known functions from R p → on R θ = ( 1 , , θ L θ q T ) R⊂Θ∈ q . Moreover, the errors are assumed to be a being 0, common variance 2σ and density f and −α mixing stationary sequence with mean  are independent with X X , ,1 L − i pi − ,{ jε j = i , i ,1 } L+ . series models is very meaningful. The consideration for asymptotic properties of variance estimators of the error in time In researching result [2] it is derived asymptotic normality of the Bickel- Rosenblatt test statistic, based on the integrated squared error of the nonparametric error density estimators and the null error density, in nonlinear autoregressive time series models with i.i.d. errors. In researching result [1] it is considered that the variance estimator, based on the n rate for the error variance and asymptotic distribution of residuals, is consistent with estimator is normal in case with i.i.d. errors. We extend the model to more general case with −α mixing errors and derive asymptotic normal of variance estimator. － 3 － Journal of Kim Il Sung University (Natural Science) No. 3 Juche103(2014) Assumption 1 Errors are there exists −α mixing stationary sequences with mean being 0, common 1( ετ ) +− = o ( δ /8> ( 2 iε has a spectral density function that a bounded and that such )( τα ε . ) ) ,0>ε ∞<+δε 4) (E i variance 2σ . For any Suppose that and continuance. Assumption 2 When r θ y )( = the spectral radius of A is <Aρ 1) ( Assumption 3 Let U p ,1 , L= kjU θ ∈ , , ∈ Ry , q ⊂Θ⊂ qR y o (|| ||) T + is defined as , e ya 2 . Where je is the j th unit vector of qR .  be any opened neighbored of θ , we assume that, for all eaA ,( 1 R∈ − ) 1 L , = e p , , q ≤ M 1 y )( (2) ≤ M 2 y )( (3) r )( y ∂ θ θ ∂ j 2 r y )( ∂ θ θθ ∂ ∂ k j ∂ θθ ∂ j where E 2 XM ( 1 i ≤≤ 1 , L 1, , X pi − j q ≤≤ i 1 − n For , set ) +∞< E, 2 XM ( 2 , , L i 1 − X pi − ) +∞< . Y ij = Xr ( , i 1 − , X L pi − ) .            (4) Then iε and ,ˆ( ˆ θθ L= 1 Let Y 1 , j Y , 2 )ˆ qθ j , , Y ij L are independent each other.  be an estimator for θ satisfying the iterated logarithm law. Therefore, there exists a constant ( < C0 ∞< ) such that lim sup n log(log n ) ˆ| θθ ≤− | C (5) where ˆ| θθ | =− ˆ( θθ − j j 2) . q ∑ j 1 = The above assumption on θˆ is satisfied for least square estimator under certain conditions. Based on the estimator θˆ , we define the residuals X = i Then we define the estimator 2ˆσ for 2σ as follows; ( Xr ˆ θ ˆ ε i L pi − X ), − = 1 − i , , i ,2,1 , L n (6) 2 ˆ σ = n 1 ∑ n 1 i = 2 ˆ ε i (7) The asymptotic properties of 2ˆσ are given as follows. Theorem 1 Under the assumptions 1_3 and (5), we have ) ⎞ ⎟ ⎠ log(log n n 1 = ∑ n 1 i = 2 ˆ σ 2 ε i ⎛ ⎜ ⎝ O + n p － 4 － (8) Asymptotic Properties of Variance Estimator in Nonlinear Autoregressive Time Series … further, under the assumption 1 1Eε ∞<4 , we have 2 2 ˆ( −σσ ) n = pO )1( .             (9) Proof By model (1) and formula (6), it follows for any i = ,2,1 L , n ˆ = εε i i − ˆ( − θθ j j ) Y ij + ˆ( ˆ)( θθθθ j k k − − j ) Z ijk .      (10) q ∑ j 1 = 1 2 q q ∑ ∑ j 1 = k 1 = Combining (10) with the definition of 2ˆσ in (7), we obtain that 2 ˆ σ = 1 n n ∑ i 1 = 2 ε i + 1 n q n ⎡ ∑ ∑ ⎢ ⎢ ⎣ 1 = 1 = i j ˆ( θθ − j j Y ) ij 2 ⎤ ⎥ ⎥ ⎦ + 1 n 4 q n q ⎡ ∑ ∑ ∑ ⎢ ⎢ ⎣ 1 = 1 = 1 = k i j ˆ)( ˆ( θθθθ k k j − − j ) Z ijk − 2 ⎤ ⎥ ⎥ ⎦ ˆ( εθθ j i Y ) ij − j − ˆ( ˆ)( θθθθ k k j − − j ) Z ε i ijk + (11) − 2 n n q ∑∑ i 1 = j 1 = + 1 n n q ⎡ ∑ ∑ ⎢ ⎢ ⎣ 1 = 1 = i j n q q ∑ ∑ ∑ i 1 = j 1 1 k = = 1 n q ⎤ ⎥ ⎥ ⎦ ⎡ ⎢ ⎢ ⎣ q ∑ ∑ l 1 = k 1 = ˆ( θθ − j j Y ) ij ˆ( ˆ)( θθθθ l k k − − l ) Z ilk . ⎤ ⎥ ⎥ ⎦ qR Since θˆ is a strong consistent estimator of θ and U ⊂Θ⊂ is an opened neighborhood of θ , ˆ θ Thus, we can use assumption 1 to obtain :0 >∀>∃ Nn N , *θ U , ∈ Nn >∀ ∈ 1, U a.( i ≤≤ . s.) n 1, ≤ j , k ≤ q , (E Z 2 ijk E) ≤ 2 XM ( 2 , L , X i 1 − pi − ) ∞< , |(E ZY ij ikl |) ≤ E 2 XM ( 1 , , L i 1 − X pi − E) 2 XM ( 2 , , L i 1 − X pi − ) ∞< , 1 − Using above equations and the strong stationary of ijk i (E 2 ε i ()E Z 2 ijk ) ≤ σ E 2 XM ( 2 (E ε i Z ) ≤ , , X ) ∞< . pi − L { iX , and Markov’s inequality, it } follows that ε i Z ijk = )( nO p .             (12) n ∑ i 1 = n ∑ i 1 = n ∑ i 1 = Z 2 ijk = )( nO p ZY ij ikl = )( nO p ⎫ ⎪ ⎪ ⎪⎪ ⎬ ⎪ ⎪ ⎪ ⎪ ⎭ 2 ⎤ ⎥ ⎥ ⎦ ≤ q n q q ∑∑ 1 1 k j = = 1 n q q ∑∑ j k 1 1 = = － 5 － By (5) and (12), we have 1 n ⎡ q q n ∑ ∑∑ ⎢ ⎢ ⎣ 1 1 1 i k j = = = 1 n n q q ∑∑∑ i 1 = j k 1 1 = = ˆ)( ˆ( θθθθ k k j − − j ) Z ijk ˆ( ˆ() 2 θθθθ k k − − j j 2 ) Z 2 ijk = O p n ∑ 1 i = ⎛ ⎜ ⎜ ⎝ (log(log 2 n n ˆ)( ˆ( θθθθ j k k − − j ) Z ε i ijk = ˆ( ˆ)( θθθθ k k j − − j Z ε i ijk = O p n ) ∑ i 1 = n ) log(log n ⎛ ⎜ ⎝ ⎞ ⎟ ⎠ 2 )) ⎞ ⎟ ⎟ ⎠ (13) (14) Journal of Kim Il Sung University (Natural Science) No. 3 Juche103(2014) 1 n ⎡ q n ∑ ∑ ⎢ ⎢ ⎣ 1 1 j i = = ⎤ ⎥ ⎥ ⎦ ⎡ ⎢ ⎢ ⎣ q q ∑ ∑ 1 1 k l = = ˆ( − θθ j j ) Y ij ˆ( ˆ)( θθθθ l k k − − l ) Z ilk = = 1 n q q q ∑∑ ∑ 1 1 1 k l j = = = ˆ( θθθθθθ − k ˆ)( k ˆ)( l − − j l j ) ZY ij ilk = O p (15) 2/3 ⎛ ⎜ ⎜ ⎝ (log(log n 2/3 )) n . ⎞ ⎟ ⎟ ⎠ ⎤ ⎥ ⎥ ⎦ n ∑ 1 i = And by assumption 2 and (5), we have 1 n ⎡ q n ∑ ∑ ⎢ ⎢ ⎣ 1 1 j i = = ˆ( − θθ j j Y ) ij = O p n ) log(log n ⎛ ⎜ ⎝ ⎞ ⎟ ⎠ 2 ⎤ ⎥ ⎥ ⎦ .          (16) In fact, by assumption 2 and the definition of ijY in (4), we have Thus, using the strong stationary of (E 2 Y ij E) ≤ i , , ) 1 − X L 2 XM ( 1 { iX and Markov’s inequality, it follows that  n ∞< pi − . } .              (17) 2 ij =∑ Y nO )( p i 1 = By Cauchy-Schwarz inequality, we have that 2 ˆ( − θθ j j ⎡ q ∑ ⎢ ⎢ ⎣ j 1 = Combining (5) with (7), we obtain that (16) holds. Now, we should estimate the end term in (11). Under assumption 2 and (5), we have that q ∑ j 1 = ) Y ij ⎤ ⎥ ⎥ ⎦ ≤ ˆ( θθ − j j 2 ) q 2 ∑ Y ij j 1 = . 1 n q n ∑∑ 1 1 j i = = ˆ( εθθ j i Y ij − ) j = O p ⎛ ⎜ ⎜ ⎝ q ) log(log n ) n ⎞ ⎟ ⎟ ⎠ ( , i εε and } { iX , we obtain that  2 ⎞ ⎟ ⎟⎟ ⎠ ∑ ⎛ ⎜ ⎜⎜ ⎝ Y ij ε i E 1 = n i = n ∑ i 1 = (E 2 2 Y ε i ij ) = nO )( . In fact, using the fact that Yij ( j ,2,1 L= , ) , 1 L+i are independent and the strong stationary of . (18) By this formula Chebyshev’s inequality, it is easy to see that ∑ ε i Y ij = ( nO p 2/1 ) . n i 1 = Combining this with (5), it follows that 1 n q n ∑∑ 1 1 j i = = ˆ( εθθ j i Y ) ij − j = ˆ( θθ − j j ) Y ε i ij = O p 1 n q ∑ 1 j = n ∑ 1 i = ⎛ ⎜ ⎜ ⎝ n ) log(log n . ⎞ ⎟ ⎟ ⎠ Therefore, we obtain (18). Thus, by (11), (13)－(16), (18), we can conclude that (8) holds. If errors −α mixing stationary sequences, according to the ( iε satisfy assumption 1 as ) central limit theorem for ( 2 iε we have that ) 2∑ 2 σε = i + nO ( p 2/1 − .) Combining this with (8), we obtain (9).□ 1 n n i 1 = － 6 － Asymptotic Properties of Variance Estimator in Nonlinear Autoregressive Time Series … Theorem 2 Under all assumptions of theorem 1, we have that in distribution 2 2 ˆ( ) →− σσ .           (19) (Var ,0( N )) n 2 ε 1 Proof Using (8), it follows that 2 2 ˆ( − σσ ) n = n 1 n ⎛ ⎜ ⎜ ⎝ n ∑ i 1 = 2 − σε 2 i ⎞ ⎟ +⎟ ⎠ O p ⎛ ⎜⎜ ⎝ log(log n ) n Using assumption 1 for −α mixing sequences n n 2 i ⎛ ⎜ ⎜ ⎝ 1 n ∑ 2 − σε ⎞ =⎟⎟ ⎠ ( 2 iε , central limit theorem is established, .  (20) ⎞ ⎟ +⎟ ⎠ )1( O 1 = ) p i so we have that n 1 n ⎛ ⎜ ⎜ ⎝ n i 1 = 2 σε −∑ 2 i ⎞ ⎟ →⎟ ⎠ N ,0( (Var 2 ε 1 )) .          (21) Combining (20) with (21), we have shown that (19) holds.□  References [1] Fu Xia Cheng; Journal of Statistical Planning and inference, 141, 1588, 2011. [2] F. Cheng et al.; Statistics and Probability Letters, 78, 50, 2008. － 7 －