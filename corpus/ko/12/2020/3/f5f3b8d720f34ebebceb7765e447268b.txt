김일성종합대학학보 정보과학 주체109(2020)년 제66권  제3호 불균형자료에 대한 초학습기계실현에서 확률비용함수를 리용하기 위한 한가지 방법 김훈, 주일령 경애하는 최고령도자 동지께서는 다음과 같이 말씀하시였다. 《우리는 가까운 앞날에 전반적인 과학기술분야에서 세계를 디디고 올라설수 있다는 배심을 가지고 첨단돌파의 기적들을 련이어 창조하여야 합니다.》 초학습기계(ELM)[1, 2]는 일반화된 단층앞방향망(SLFNs)에 대한 새로운 학습알고리듬 으로서 다른 인공신경망보다 실행이 간단하고 빠르므로 많은 실천에 응용된다. 그러나 자료의 분포를 정확하게 표현하지 못하므로 자료클라스들사이의 분류정확도가 높지 못하 고 불균형자료에 대하여 소수클라스쪽으로 치우치므로 출입체계와 같은 인식과제에 적당 하지 않다.[3, 4] 불균형자료문제에 대한 분류를 잘하지 못하는 초기ELM의 결함을 극복하기 위하여 무 게붙은 ELM[3]이 제안되였다. 그 방법의 본질은 다수클라스의 영향을 약화시키고 동시에 소수클라스의 영향을 강화하도록 매 견본에 정확한 무게를 할당하는것이다. 무게붙은 ELM에서 매 훈련견본은 그것이 속하는 클라스의 견본개수에 따라 계산된 무게를 할당하였다. 그래서 무게붙은 ELM은 초기ELM의 우점을 유지하면서 불균형자료 에 대한 분류성능을 개선하였다. 무게붙은 ELM에서는 모든 훈련견본 NN × 대각선무게행렬 W를 정 의하고 소수클라스의 원소 iiW 는 다수클라스의 원소들에 대응하는 무게보다 상대적으로 더 크게 설정하였다. 결과 소수클라스들이 강조되고 동시에 다수클 라스들이 약화되였다. iix 에 대응하는 무게 ix 와 관련된 1개 대각선무게행렬 W를 고려하면 무게붙은 ELM은 다음식으로 표시할수 있다. L WELM = 2 β + C ( W ii × ξ 2 ) (1) 1 2 1 2 N ∑ i 1 = xh ( i ) β = t T i − T ξ i , i (cid:34)= ,1 , N THβ = = * + WHH T LN < 때일 T H I C ⎛ ⎜ ⎝ ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ I C ⎛ ⎜ ⎝ + T WHH ⎞ ⎟ ⎠ 1 − WT , ⎞ ⎟ ⎠ 1 − TWH T , LN ≥ 때일 (2) 이다. 그리고 N은 벡토르모임, T는 벡토르모임에 대응하는 클라스표식벡토르, ti는 벡토르 모임에 대응하는 클라스표식, H는 숨은층출력행렬, β는 숨은층출력무게행렬, h(xi)는 숨은 층출력행렬의 원소, ξ 는 훈련견본 xi에 대응한 N개의 출력마디의 훈련오유벡토르, C는 훈련오유최소화와 클라스들사이 거리최대화의 균형파라메터이다. 여기서 이고 － 24 － 종합대학학보 정보과학 주체109(2020)년 제66권 제3호 제안한 2개의 무게구성은 다음과 같다. W 1 : W ii = W 2 : W ii = 1 t # i 618.0 t # 1 t # i i ⎧ ⎪⎪ ⎨ ⎪ ⎪ ⎩ , , 여기서 균견본개수이다. # t i > AVG (# t 때일 ) i (3) # t i ≤ AVG (# t 때일 ) i it# 는 i째 클라스에 속하는 견본들의 개수이며 AVG (# it ) 는 모든 클라스들의 평 론문에서는 훈련견본자료가 매 클라스에 속할 확률을 고려하여 확률비용함수를 정의 하고 이 확률비용함수를 리용하여 ELM문제의 최량풀이를 구하였다. 입력훈련자료를 주어진 훈련견본자료가 매 클라스에 속할 확률값으로 이루어진 1개 벡토르로 넘기고 매 훈련견본자료가 클라스에 정확히 속할 정도를 평가하기 위하여 확률 비용함수를 정의하였다. M클라스들로 이루어진 훈련모임을 ), i 이때 표준ELM알고리듬에 관한 이 훈련모임의 숨은층출력행렬 H는 다음과 같다. 으로 표시하자. (cid:34)= ,1 tx , i N ( i , T aF ( 1 + b 1 ) ⋅ x 1 (cid:35) H = aF ( T L ⋅ + b L ) x 1 (cid:35) T aF ( 1 ⋅ x N + b 1 ) aF ( T L ⋅ x N + b L ) (cid:34) (cid:37) (cid:34) ⎡ ⎢ ⎢ ⎢ ⎣ ⎤ ⎥ ⎥ ⎥ ⎦ LN × (4) 여기서 F는 숨은층의 적용함수이다. (cid:34)= ,2,1 편리하게 하기 위하여 , iHi ( T aF ( ⋅ 1 N ) T 2 그러면 H i = ( x i + aFb ), 1 ( ⋅ x i + b 2 ), (cid:34) , aF ( ⋅ x i + b L )) (5) T L 를 다음과 같이 정의하자. H = ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ 1 2 H ⎤ ⎥ H ⎥ ⎥ (cid:35) ⎥ LNNH ⎦ × 이다. 두 분류 ELM분류기에서 출력무게행렬 β는 ML × 행렬이다. β = ( BB , 1 2 , (cid:34) , ) MLMB × 여기서 [ = B i BB , i 1 2 B 따라서 표준ELM의 출력은 다음과 같다. M (cid:34) (cid:34) ,1 = Li i , , , i 이다. T ,] H β = BH 11 BH 12 (cid:35) ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ (cid:34) BH 21 (cid:34) BH 22 (cid:37)(cid:35) M M BH 1 BH 2 (cid:35) ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ MNMN BH × = ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ f 1 f 2 (cid:35) f N ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ BHBH 2 N N 1 (cid:34) f i = ( BHBH , 1 2 i i , (cid:34) , BH Mi ), i = ,2,1 (cid:34) , N 여기서 BH i j BH i j e e M ∑ j 1 = M ∑ j 1 = 이다. 불균형자료에 대한 초학습기계실현에서 확률비용함수를 리용하기 위한… － 25 － 이때 표준ELM의 최량화공식은 다음과 같다. L ELM = 2 β + C 1 2 1 2 N ∑ i 1 = 2 ξ i (6) 여기서 첫항은 여분거리와 관련되며 둘째 항은 훈련오유와 관련된다. 우리는 식 (6)의 둘째 항을 주어진 훈련견본이 매 클라스에 속할 확률과 관련된 값 으로 변화시켰다. 이를 위해 다음과 같은 확률함수를 리용한다. ( xp i , j , ) β = , i = ,1 (cid:34) , , jN = ,1 (cid:34) , M (7) 0 < ( xp i , j , β ,1) < ( xp i , j , β ,1) = j = ,1 (cid:34) , M 이 값들은 훈련견본 ix 가 매 클라스에 속할 확률로 볼수 있다. 다시말하여 매 훈련 견본이 임의의 클라스에 속할 확률은 이 함수에 의하여 얻어질수 있다. 그러므로 확률비용함수를 다음과 같이 정의하였다. L ) ( β = 2 β − C 1 2 1 N N M ∑∑ i 1 = j 1 = t ij log ( xp i , j , ) β (8) 여기서 훈련견본 ix 에 대응하는 목표벡토르  = [ t t t , 1 i 의 매 원소들은 클라스 k에 속하면 i (cid:34) , t i 2 1=ikt i = iM ], ,2,1 , N 로, 그렇지 않으면 (cid:34) , 0=ikt 으로 설정된다. (βL 의 최소값을 구하기 위하여 그라디엔트하강법, L-BFGS와 같은 반복최량화알고 ) 리듬을 리용한다. 이때 그것의 도함수는 다음과 같다. ∇ B j BL = − j [ ( tH i ij − ( xp i , j , β ,))] j (cid:34)= ,1 , M (9) C N N ∑ i 1 = i , i t ( ) ), (cid:34)= ,1 β에 관한 (βL 의 최소화를 진행하여 최량출력무게행렬을 얻는다.  확률비용함수에 기초한 초학습기계의 동작알고리듬은 다음과 같다. 입력: 훈련자료 x N i 출력: 출력무게 β 걸음 １ 우연적으로 입력파라메터 , iba i i iH 를 계산한다. 걸음 ２ 식 (4), (5)로 걸음 ３ 식 (7)로 확률함수 ( xp i , ), 식 즉 확률비용함수식 (8)을 얻는다. , 숨은층의 능동함수 F 들을 할당한다. (cid:34)=β (cid:34)= ,1 M ,1 L j j , , , , , 을 정의하고 이를 리용하여 최량화 걸음 ４ 반복최량화알고리듬을 리용하여 확률비용함수식 (8)의 최량풀이 β 를 얻 걸음 ５ β를 출력무게로 하여 출력한다. 는다. － 26 － 종합대학학보 정보과학 주체109(2020)년 제66권 제3호 맺 는 말 참 고 문 헌 훈련견본자료가 매 클라스에 속할 확률을 고려한 확률비용함수를 정의하고 이 확률 비용함수를 리용하여 ELM최량화문제를 정식화하였으며 이 방법으로 두 분류문제를 풀 기 위한 학습알고리듬을 제안하였다. [1] G.-B. Huang et al.; Neurocomputing, 70, 1, 489, 2006. [2] G. B. Huang et al.; IEEE Computational Intelligence Magazine, 10, 2, 18, 2015. [3] A. Iosifidis et al.; IEEE Transactions on Cybernetics, 46, 1, 311, 2016. [4] E. A. Garcia et al.; IEEE Trans. Knowl. Data Eng., 21, 9, 1263, 2009. 주체109(2020)년 5월 5일 원고접수 A Study on a Method for Using Probability Cost Function in Extreme Learning Machine Realization on Imbalance Data Kim Hun, Ju Il Ryong In this paper, we propose a modified ELM algorithm that defined by probability cost function where probability which training samples belong to each class is found. Keywords: extreme learning machine, imbalance learning, probability cost function