김일성종합대학학보 수 학 주체110(2021)년 제67권 제1호 재귀중첩신경망을 리용하여 조선어문장을 분류하기 위한 한가지 방법 심천룡, 리명철 우리는 재귀중첩신경망(Recurrent Convolutional Neural Network)의 구조를 변경하여 조 선어문장을 분류하기 위한 방법을 제기하고 실험을 통하여 그 효과성을 검증하였다. 선행연구[1]에서는 LSTM(Long-Short Term Memory)에 기초한 모형을 리용하여 단어의 순서정보를 얻어내는 방법으로 분류와 의미해석성능을 높였으며 선행연구[2]에서는 중첩 신경망(CNN)을 리용하여 단어들사이의 국부적관계와 개별적인 단어의 특성을 얻어내였 으며 계산의 효률성을 보장하였다. 그러나 선행연구[1]에서 제안한 모형은 문장이 길어질 수록 단어들사이의 호상순서관계를 학습하기 힘들어하는 결함이 있으며 선행연구[2]의 모형은 단어들의 순서관계를 효과적으로 반영할수 없는 결함을 가지고있다. 론문에서는 이러한 결함들을 극복하고 조선어문장을 분류하기 위하여 재귀중첩신경 망[3]을 리용하였다. 조선어문장의 합리적인 표현을 얻기 위하여 재귀중첩신경망의 구조 를 변경하였으며 실험을 통하여 조선어문장분류에서 제안한 망이 종전의 재귀중첩신경망 [4], 1차원중첩신경망[2], LSTM[1]보다 분류정확도를 높일수 있다는것을 확인하였다. １．조선어문장분류를 위한 망의 구조 일반적으로 본문분류에 재귀중첩신경망을 리용할 때 입력값은 ( ww , 1 2 , (cid:34) , nw ) 과 같 은 하나의 문장이다. 이때 재귀중첩신경망에서는 문장의 단어들에 해당한 벡토르표현을 쌍방향재귀신경망 을 통과한 문맥의 벡토르표현과 련결하고 1차원중첩층을 통과시킨다. 1차원중첩층의 출력값에 최대선택층을 적용하여 해당 문장의 최종적인 벡토르표현을 얻고 전결합층을 거쳐 분류를 진행한다. 론문에서는 길이가 긴 조선어문장들의 표현도 효과적으로 얻을수 있도록 우의 재귀 중첩신경망의 구조에서 쌍방향일반재귀신경망을 긴 문장에서 단어순서관계를 학습할수 있는 쌍방향LSTM으로 교체하였으며 창문크기가 1, 3, 5, 9인 여러개의 중첩층들을 리용하 여 각이한 창문크기안의 단어들의 순서관계를 동시에 학습할수 있게 하였다. 이때 중첩층들의 활성화함수로는 ReLU(Rectified Linear Unit)[2]를 리용하였다. 또한 최대선택층과 평균선택층을 모두 리용하였으며 효과성을 실험을 통하여 검증하 였다. 론문에서 제안한 망의 구조는 그림 1과 같다. － 48 － 종합대학학보 수학 주체110(2021)년 제67권 제1호 론문에서 제안한 변경된 재귀중첩신경망의 층들은 표 1과 같다. 그림 1．론문에서 제안한 망의 구조 층이름 입력층 벡토르표현층 쌍방향LSTM 련결층(하단) 중첩층 (1) 중첩층 (3) 중첩층 (5) 중첩층 (9) 선택층(최대, 평균) 련결(상단) 전결합층 표 １. 론문에서 제안한 변경된 재귀중첩신경망의 층들 출력형태 [묶음크기, 문장크기] [묶음크기,문장크기, 벡토르표현차원수] [묶음크기, 문장크기, 2×LSTM출력차원수] [묶음크기, 문장크기,  벡토르표현차원수+2×LSTM출력차원수] [묶음크기, 문장크기, 중첩층출력차원수] [묶음크기, 문장크기－2, 중첩층출력차원수] [묶음크기, 문장크기－4, 중첩층출력차원수] [묶음크기, 문장크기－8, 중첩층출력차원수] [묶음크기, 중첩층출력차원수] [묶음크기, 중첩층출력차원수×8] [묶음크기, 모임개수] × ○ ○ ○ ○ × × ○ 훈련파라메터존재 × ○ ○ 론문에서는 우선 일반적인 재귀신경망대신에 LSTM을 리용하여 길이가 긴 조선어문 장에 대하여서도 보다 효과적인 표현을 얻을수 있게 하였다. 론문에서는 다음으로 종전의 창문크기가 1이던 중첩층을 창문크기가 1, 3, 5, 9인 중 첩층들로 교체하여 보다 넓은 범위의 국부적특징을 학습할수 있게 하였으며 활성화함수 로 ReLU를 리용하여 그라디엔트손실을 줄일수 있게 하였다. 그리고 마지막으로 중첩층의 출력에 최대선택층과 평균선택층을 동시에 적용하여 망 의 표현능력을 높이였으며 최대선택층과 평균선택층을 모두 리용할 때와 최대선택층만을 리용할 때의 성능비교실험을 진행하였다. 재귀중첩신경망을 리용하여 조선어문장을 분류하기 위한 한가지 방법 － 49 － 론문에서 제안한 망의 흐름도는 그림 2와 같다. 그림 2．론문에서 제안한 망의 흐름도 1－해당 문장의 표현(단어개수×벡토르표현차원수), 2－쌍방향LSTM의 출력과 문장표현의 련결, 3－창문크기가 여러가지인 중첩층의 적용, 4－선택층의 적용과 그 출력들의 련결, 5－전결합층과 유연최대함수의 출력 론문에서는 Adam알고리듬[5]의 모멘트감쇠초파라메터 1β 과 척도감쇠초파라메터 2β 를 조절함으로써 모형이 빨리 수렴할수 있게 하여 학습속도를 높이였다. Adam알고리듬은 다음과 같다. ① 1( −+ )( θ ) ∇ ← m m ② ③ ④ β 1 ) ∇ θJ J θ )) s m 1( β 1 s −+← β 2 β 2 T 1β− T )) 2β− ⊗′ m −← ηθθ ← s ( ←′ 1/( 1/( m ˆ s ( )( θ ∇⊗ J )( θ θ − 1 − 10 ˆ s , ( + = ε 10 ) ε ⑤ 제안모형의 경우 RMSProp[5]보다 Adam알고리듬이 보다 좋은 훈련성능을 보여주었 으며 1β 을 기정값(0.9)보다 약간 크게 하여 모멘트의 영향을 증가시키고 2β 를 기정값 (0.999)보다 작게 하여 모형의 파라메터갱신속도를 증가시켰다. 즉 론문에서 제안한 모형 의 파라메터공간은 그라디엔트변화가 일정한 구간 다시말하여 평탄한 구간이 많았으며 이것을 해결하기 위하여 모멘트감쇠초파라메터의 값을 크게 하는 방식을 취하였다. 또한 론문에서 제안한 모형의 파라메터공간에서 파라메터들사이의 그라디엔트변화률이 심하지 않으므로 척도감쇠초파라메터의 값을 기정값보다 크게 취하여 파라메터들사이의 변화률 을 조절해주어 학습이 빨리 진행되도록 하였다. ２．실험을 위한 자료기지와 실험결과 론문에서는 조선어문장을 《신문》(보도기사), 《소설》, 《교과서》의 3개 부류로 나누기 위한 망을 구성하고 훈련자료를 준비하였다. 신문부류에서는 로동신문, 평양신문에서 나 오는 보도기사자료를 추출하여 리용하였으며 소설부류는 우리 나라 현대소설들을, 교과 서부류는 여러가지 자연과학참고서와 교과서들의 본문자료를 리용하였다. 이때 전체 자 료를 4：1의 비률로 나누어 모형의 훈련과 검사에 리용하였다. 학습은 벡토르표현층부터 마지막층까지 모든 층을 일괄적으로 학습시키는 방법으로 진행하였다. － 50 － 종합대학학보 수학 주체110(2021)년 제67권 제1호 론문에서 훈련에 리용한 모형의 초파라메터들은 표 2와 같다. 최대선택층과 평균선택층을 모 두 리용할 때와 최대선택층만을 리용 할 때의 실험결과는 표 3과 같고 론 문에서 여러가지 중첩층을 동시에 리 용할 때의 실험결과는 표 4와 같다. 표 ２．훈련에 리용한 초파라메터 초파라메터이름 묶음크기 단어선택개수 및 벡토르표현의 차원수 쌍방향LSTM출력차원수 실험을 통하여 최대선택층과 평균선택층을 동시에 리용하여 모 형의 정확도를 향상시킬수 있다는 것을 검증하였다. 중첩층의 출력차원수 중첩층의 창문크기 Adam알고리듬의 초파라메터설정 , ( 1 ββ 2 ) 값 32 100 000, 100 128 128 1, 3, 5, 9  0.95, 0.985 표 ３．최대선택층과 평균선택층을 모두 리용할 때의 실험결과 분류 최대선택층과 평균선택층의 리용 최대선택층의 리용 정확도/% 91.8 90.9 분류(창문크기) 1 9 1, 3, 5, 9 정확도/% 89.85 90.82 91.80 표 ４． 여러가지 중첩층을 동시에 리용할 때의 실험결과 변경된 재귀중첩신경망과 종전의 재귀중첩신경망을 비롯한 여러 망들을 리용하여 학습시켜 비교한 결과는 표 5와 같다. 이때 평가는 검 사자료기지에서의 정확도로 진행하였다. 표 ５．론문에서 제안한 모형과 선행모형들의 비교 정확도/% 91.80 89.32 88.57 88.07 모형 변경된 재귀중첩신경망(제안모형) 재귀중첩신경망(종전의 모형 RCNN) 1차원중첩신경망(CNN) LSTM 론문에서 제안한 모형을 실험을 통 하여 검증함으로써 재귀중첩신경망의 구 조를 변경하여 조선어문장분류에 리용하 면 종전의 모형들에 비하여 좋은 결과를 얻을수 있다는것을 확인하였다. 참 고 문 헌 [1] Fei Liu et al.; NAACL, 2, 278, 2018. [2] Rie Johnson et al.; ACL, 1, 562, 2017. [3] Siwei Lai et al.; AAAI, 333, 2267, 2015. [4] Peixiang Zhong et al.; arXiv:1902.07867v2, 2019. [5] S. J. Reddi et al.; arXiv:1904.09237, 2019. 주체109(2020)년 9월 5일 원고접수 A Method for Korean Sentence Classification Using Recurrent Convolutional Neural Network Sim Chon Ryong, Ri Myong Chol We propose a new network construction method for Korean sentence classification by modifying the recurrent convolutional neural network structure and verify the accuracy through experiments.  The results demonstrate that our new method is more effective for Korean sentence classification. Keywords: Korean sentence classification, recurrent convolutional neural network